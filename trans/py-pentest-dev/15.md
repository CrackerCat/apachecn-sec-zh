# 第 5 章 Web 服务器和 Web 应用程序的脚印

到目前为止，我们已经阅读了从数据链路层到传输层的四章。现在，我们转到应用层渗透测试。在本章中，我们将介绍以下主题：

*   web 服务器的脚打印概念
*   介绍信息收集
*   HTTP 头检查
*   Ulsoup 从 Smathhois.com 收集网站信息
*   网站的横幅抓取
*   强化 web 服务器

# web 服务器的脚印概念

渗透测试的概念不能在一个步骤中解释或执行；因此，它被分为几个步骤。脚印是 pentesting 的第一步，攻击者试图收集有关目标的信息。在当今世界，电子商务正在快速发展。因此，web 服务器成为黑客的主要目标。为了攻击 web 服务器，我们必须首先了解 web 服务器的含义。我们还需要了解 web 服务器托管软件、托管操作系统以及 web 服务器上运行的应用程序。在获得这些信息后，我们可以建立我们的漏洞。获取此信息称为 web 服务器的脚踏打印。

# 介绍信息收集

在本节中，我们将尝试使用错误处理技术收集有关 web 服务器上运行的 web 软件、操作系统和应用程序的信息。从黑客的角度来看，从错误处理中收集信息并不是很有用。然而，从 pentester 的角度来看，这是非常重要的，因为在提交给客户的 pentesting 最终报告中，您必须指定错误处理技术。

错误处理背后的逻辑是尝试在 web 服务器中生成错误，返回代码 404，并查看错误页面的输出。我已经写了一个小代码来获得输出。我们将逐行浏览以下代码：

```
import re
import random
import urllib
url1 = raw_input("Enter the URL ")
u = chr(random.randint(97,122))
url2 = url1+u
http_r = urllib.urlopen(url2)

content= http_r.read()flag =0
i=0
list1 = []
a_tag = "<*address>"
file_text = open("result.txt",'a')

while flag ==0:
  if http_r.code == 404:
    file_text.write("--------------")
    file_text.write(url1)
    file_text.write("--------------\n")

    file_text.write(content)
    for match in re.finditer(a_tag,content):

      i=i+1
      s= match.start()
      e= match.end()
      list1.append(s)
      list1.append(e)
    if (i>0):
      print "Coding is not good"
    if len(list1)>0:
      a= list1[1]
      b= list1[2]

      print content[a:b]
    else:
      print "error handling seems ok"
    flag =1
  elif http_r.code == 200:
    print "Web page is using custom Error page"
    break
```

我已经导入了三个模块`re`、`random`和`urllib`，分别负责正则表达式生成随机数和 URL 相关活动。`url1 = raw_input("Enter the URL ")`语句要求提供网站的 URL，并将此 URL 存储在`url1`变量中。接下来，`u = chr(random.randint(97,122))`语句创建一个随机字符。下一条语句将此字符添加到 URL 中，并将其存储在`url2`变量中。然后，`http_r = urllib.urlopen(url2)`语句打开`url2`页面，该页面存储在`http_r`变量中。`content= http_r.read()`语句将网页的所有内容传输到`content`变量中：

```
flag =0
i=0
list1 = []
a_tag = "<*address>"
file_text = open("result.txt",'a')
```

前面的代码定义了变量标志**`i`和一个空列表，其意义我们将在后面讨论。`a_tag`变量取一个值`"<*address>"`。`file_text`变量是以追加模式打开`result.txt`文件的文件对象。`result.txt`文件存储结果。`while flag ==0:`语句表示我们希望 while 循环至少运行一次。`http_r.code`语句从 web 服务器返回状态代码。如果找不到该页面，它将返回 404 代码：**

 **```
file_text.write("--------------")
file_text.write(url1)
file_text.write("--------------\n")

file_text.write(content)
```

前面的代码将页面的输出写入`result.txt`文件。

`for match in re.finditer(a_tag,content):`语句在错误页面中找到`a_tag`模式，即`<address>`标记，因为我们对`<address> </address>`标记之间的信息感兴趣。`s= match.start()`和`e= match.end()`语句表示`<address>`标签和`list1.append(s)`的起点和终点。`list1.append(e)`语句将这些点存储在列表中，以便我们以后可以使用这些点。`i`变量变得大于 0，这表示错误页面中存在`<address>`标记。这意味着代码不好。`if len(list1)>0:`语句表示，如果列表至少有一个元素，那么变量`a`和`b`将是关注点。下图显示了这些关注点：

![Introducing information gathering](graphics/8583OT_05_01.jpg)

获取地址标记值

`print content[a:b]`语句读取`a`和`b`点之间的输出，并设置`flag = 1`以中断`while`循环。`elif http_r.code == 200:`语句表示如果 HTTP 状态码为 200，则会打印`"Web page is using custom Error page"`消息。在这种情况下，如果返回错误页面的代码 200，则表示该错误正在由自定义页面处理。

现在是运行输出的时候了，我们将运行它两次。

服务器签名打开和关闭时的输出如下：

![Introducing information gathering](graphics/8583OT_05_02.jpg)

程序的两个输出

前面的屏幕截图显示了服务器签名打开时的输出。通过查看此输出，可以说 web 软件为**Apache**，版本为**2.2.3**，操作系统为 Red Hat。在下一个输出中，没有来自服务器的信息意味着服务器签名已关闭。有时有人会使用 web 应用程序防火墙，比如 mod security，它会提供一个虚假的服务器签名。在这种情况下，您需要检查`result.txt`文件以获得完整详细的输出。我们来查看`result.txt`的输出，如下图所示：

![Introducing information gathering](graphics/8583OT_05_03.jpg)

result.txt 的输出

当存在多个 URL 时，您可以列出所有这些 URL 并将其提供给程序，该文件将包含所有 URL 的输出。

## 检查 HTTP 报头

通过查看网页的标题，可以得到相同的输出。有时，可以通过编程更改服务器错误输出。但是，检查标头可能会提供大量信息。一个非常小的代码可以为您提供一些非常详细的信息，如下所示：

```
import urllib
url1 = raw_input("Enter the URL ")
http_r = urllib.urlopen(url1)
if http_r.code == 200:
  print http_r.headers
```

`print http_r.headers`语句提供 web 服务器的头。

输出如下：

![Checking the HTTP header](graphics/8583OT_05_04.jpg)

获取标题信息

您会注意到我们从程序中获取了两个输出。在第一次输出中，我们输入了[http://www.juggyboy.com/](http://www.juggyboy.com/) 作为 URL。该程序提供了很多有趣的信息，如**服务器：微软 IIS/6.0**和**X-Powered-By:ASP.NET**；推断网站托管在 Windows 机器上，web 软件为**IIS 6.0**，而**ASP.NET**用于 web 应用程序编程。

在第二个输出中，我提供了本地机器的 IP 地址，即`http://192.168.0.5/`。该程序透露了一些秘密信息，例如 web 软件是 Apache2.2.3，它运行在 Red Hat 机器上，PHP 5.1 用于 web 应用程序编程。通过这种方式，您可以获得有关操作系统、web 服务器软件和 web 应用程序的信息。

现在，让我们看看如果服务器签名关闭，我们将得到什么输出：

![Checking the HTTP header](graphics/8583OT_05_05.jpg)

当服务器签名关闭时

从前面的输出中，我们可以看到 Apache 正在运行。但是，它既不显示版本也不显示操作系统。对于 web 应用程序编程，使用了 PHP，但有时，输出不显示编程语言。为此，您必须解析网页以获得任何有用的信息，例如超链接。

如果要获取表头的详细信息，请打开表头的`dir`，如下代码所示：

```
 >>> import urllib
>>> http_r = urllib.urlopen("http://192.168.0.5/")
>>> dir(http_r.headers)
['__contains__', '__delitem__', '__doc__', '__getitem__', '__init__', '__iter__', '__len__', '__module__', '__setitem__', '__str__', 'addcontinue', 'addheader', 'dict', 'encodingheader', 'fp', 'get', 'getaddr', 'getaddrlist', 'getallmatchingheaders', 'getdate', 'getdate_tz', 'getencoding', 'getfirstmatchingheader', 'getheader', 'getheaders', 'getmaintype', 'getparam', 'getparamnames', 'getplist', 'getrawheader', 'getsubtype', 'gettype', 'has_key', 'headers', 'iscomment', 'isheader', 'islast', 'items', 'keys', 'maintype', 'parseplist', 'parsetype', 'plist', 'plisttext', 'readheaders', 'rewindbody', 'seekable', 'setdefault', 'startofbody', 'startofheaders', 'status', 'subtype', 'type', 'typeheader', 'unixfrom', 'values']
>>> 
>>> http_r.headers.type
'text/html'
>>> http_r.headers.typeheader
'text/html; charset=UTF-8'
>>>
```**  **# 用户组从 SmartWhois 收集网站信息

考虑一个你想从网页中收集所有超链接的情况。在本节中，我们将通过编程来实现这一点。另一方面，这也可以通过查看网页的视图源手动完成。然而，这需要一些时间。

因此，让我们了解一个名为 BeautifulSoup 的非常漂亮的解析器。此解析器来自第三方源代码，非常易于使用。在我们的代码中，我们将使用 BeautifulSoup 的版本 4。

要求是 HTML 页面和超链接的标题。

代码如下：

```
import urllib
from bs4 import BeautifulSoup
url = raw_input("Enter the URL ")
ht= urllib.urlopen(url)
html_page = ht.read()
b_object = BeautifulSoup(html_page)
print b_object.title
print b_object.title.text
for link in b_object.find_all('a'):
  print(link.get('href'))
```

`from bs4 import BeautifulSoup`语句用于导入 BeautifulSoup 库。`url`变量存储网站的 URL，`urllib.urlopen(url)`打开网页，`ht.read()`函数存储网页。`html_page = ht.read()`语句将网页分配给`html_page`变量。为了更好地理解，我们使用了这个变量。在`b_object = BeautifulSoup(html_page)`语句中，创建了`b_object`的对象。下一条语句打印带有标记和不带标记的标题名。下一个`b_object.find_all('a')`语句保存所有超链接。下一行仅打印超链接部分。程序的输出将清除所有疑问，如以下屏幕截图所示：

![Information gathering of a website from SmartWhois by the parser BeautifulSoup](graphics/8583OT_05_06.jpg)

所有超链接和标题

现在，您已经了解了如何使用漂亮的解析器获得超链接和标题。

在下一个代码中，我们将在 BeautifulSoup 的帮助下获得一个特定字段：

```
import urllib
from bs4 import BeautifulSoup
url = "https://www.hackthissite.org"
ht= urllib.urlopen(url)
html_page = ht.read()
b_object = BeautifulSoup(html_page)
data = b_object.find('div', id ='notice')
print data
```

前面的代码取了[https://www.hackthissite.org](https://www.hackthissite.org) 作为`url`，在下面的代码中，我们感兴趣的是找到`<div id = notice>`在哪里，如下面的截图所示：

![Information gathering of a website from SmartWhois by the parser BeautifulSoup](graphics/8583OT_05_07.jpg)

div ID 信息

现在让我们在下面的屏幕截图中看到前面代码的输出：

![Information gathering of a website from SmartWhois by the parser BeautifulSoup](graphics/8583OT_05_08.jpg)

代码的输出

考虑另一个你想收集网站信息的例子。在为特定网站收集信息的过程中，您可能使用了[http://smartwhois.com/](http://smartwhois.com/) 。通过使用 SmartWhois，您可以获得有关任何网站的有用信息，例如注册人姓名、注册人组织、名称服务器等。

在以下代码中，您将看到如何从 SmartWhois 获取信息。在信息收集的过程中，我研究了 SmartWhois，发现它的`<div class="whois">`标签保留了相关信息。以下程序将从该标记收集信息，并以可读形式保存在文件中：

```
import urllib
from bs4 import BeautifulSoup
import re
domain=raw_input("Enter the domain name ")
url = "http://smartwhois.com/whois/"+str(domain)
ht= urllib.urlopen(url)
html_page = ht.read()
b_object = BeautifulSoup(html_page)
file_text= open("who.txt",'a')
who_is = b_object.body.find('div',attrs={'class' : 'whois'})
who_is1=str(who_is)

for match in re.finditer("Domain Name:",who_is1):
      s= match.start()

lines_raw = who_is1[s:]  
lines = lines_raw.split("<br/>",150)    
i=0
for line in lines :
  file_text.writelines(line)
  file_text.writelines("\n")
  print line
  i=i+1
  if i==17 :
    break
file_text.writelines("-"*50)
file_text.writelines("\n")
file_text.close()
```

让我们分析一下`file_text= open("who.txt",'a')`语句，因为我希望您遵循前面的代码。`file_text`文件对象以追加模式打开`who.txt`文件来存储结果。`who_is = b_object.body.find('div',attrs={'class' : 'whois'})`语句生成所需的结果。但是，`who_is`并不包含字符串形式的所有数据。如果我们使用`b_object.body.find('div',attrs={'class' : 'whois'}).text`，它将输出包含标签的所有文本，但这些信息变得非常难以阅读。`who_is1=str(who_is)`语句将信息转换为字符串形式：

```
for match in re.finditer("Domain Name:",who_is1):
      s= match.start()
```

前面的代码找到了`"Domain Name:"`字符串的起点，因为我们有价值的信息在这个字符串后面。`lines_raw`变量包含`"Domain Name:"`字符串后面的信息。`lines = lines_raw.split("<br/>",150)`语句使用`<br/>`分隔符分隔行，`"lines"`变量成为一个列表。这意味着在 HTML 页面中，如果存在中断符**`</br>`**，则语句将生成一个新行，所有行将存储在名为 lines 的列表中。`i=0`变量初始化为 0，将进一步用于打印行数作为结果。下面的代码以存在于硬盘上的文件形式保存结果，并在屏幕上显示结果。****

 ****屏幕输出如下所示：

![Information gathering of a website from SmartWhois by the parser BeautifulSoup](graphics/8583OT_05_09.jpg)

SmartWhois 提供的信息

现在，让我们查看文件中的代码输出：

![Information gathering of a website from SmartWhois by the parser BeautifulSoup](graphics/8583OT_05_10.jpg)

代码在文件中的输出

### 注

您已经看到了如何从网页中获取超链接，通过使用前面的代码，您可以获取有关超链接的信息。不要停在这里；相反，请尝试在[阅读更多关于 BeautifulSoup 的 http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) 。

现在，让我们进行一个练习，将列表中的域名作为输入，并将结果写入一个文件中。****  ****# 网站的横幅抓取

在本节中，我们将获取一个网站的 HTTP 横幅。**抓取横幅**或**操作系统指纹**是确定目标 web 服务器上运行的操作系统的一种方法。在下面的程序中，我们将在我们的计算机上嗅探网站的数据包，正如我们在[第 3 章](13.html "Chapter 3. Sniffing and Penetration Testing")、*嗅探和渗透测试*中所做的那样。

横幅抓取器的代码如下所示：

```
import socket
import struct
import binascii
s = socket.socket(socket.PF_PACKET, socket.SOCK_RAW, socket.ntohs(0x0800))
while True:

  pkt  = s.recvfrom(2048)
  banner = pkt[0][54:533]
  print banner
  print "--"*40
```

由于您必须已经阅读了[第 3 章](13.html "Chapter 3. Sniffing and Penetration Testing")、*嗅探和渗透测试*，因此您应该熟悉此代码。`banner = pkt[0][54:533]`声明在这里是新的。在`pkt[0][54:]`之前，数据包包含 TCP、IP 和以太网信息。在做了一些点击追踪后，我发现抓取横幅的信息位于`[54:533]`之间，你可以通过切片`[54:540]`、`[54:545]`、`[54:530]`等进行点击追踪。

要获得输出，您必须在程序运行时在 web 浏览器中打开网站，如下图所示：

![Banner grabbing of a website](graphics/8583OT_05_11.jpg)

抢旗

因此，前面的输出显示服务器是 Microsoft IIS.6.0，ASP.NET 是正在使用的**编程语言。我们得到的信息与我们在报头检查过程中收到的信息相同。请尝试此代码，并使用不同的状态代码获取更多信息。**

 **通过使用前面的代码，您可以为自己准备信息收集报告。当我将信息收集方法应用到网站上时，我通常会发现客户犯了很多错误。在下一节中，您将看到 web 服务器上最常见的错误。**  **# 强化 web 服务器

在本节中，让我们介绍一下在 web 服务器上观察到的常见错误。我们还将讨论以下几点来强化 web 服务器：

*   始终隐藏服务器签名。
*   如果可能，设置一个伪造的服务器签名，这可能会误导攻击者。
*   处理错误。
*   尝试隐藏编程语言页面扩展，因为攻击者很难看到 web 应用程序的编程语言。
*   使用供应商提供的最新修补程序更新 web 服务器。它避免了任何利用 web 服务器的机会。至少可以保护服务器的已知漏洞。
*   不要使用第三方补丁来更新 web 服务器。第三方修补程序可能包含特洛伊木马、病毒等。
*   不要在 web 服务器上安装其他应用程序。如果安装操作系统（如 RHEL 或 Windows），请不要安装其他不必要的软件（如 Office 或 Editor），因为它们可能包含漏洞。
*   关闭除`80`和`443`之外的所有端口。
*   不要在 web 服务器上安装任何不必要的编译器，如 gcc。如果攻击者破坏了 web 服务器并希望上载可执行文件，则 ID 或 IP 可以检测到该文件。在这种情况下，攻击者将在 web 服务器上上载代码文件（以文本文件的形式），并在 web 服务器上执行该文件。此执行可能会损坏 web 服务器。
*   设置活动用户的数量限制，以防止 DDOS 攻击。
*   在 web 服务器上启用防火墙。防火墙做很多事情，比如关闭端口、过滤流量等等。

# 总结

在本章中，我们了解了 web 服务器签名的重要性，获得服务器签名是黑客攻击的第一步。亚伯拉罕·林肯曾经说过：

> *“给我六个小时砍树，我会用前四个小时磨斧头。”*

同样的情况也适用于我们的情况。在对 web 服务器发起攻击之前，最好准确地检查哪些服务正在其上运行。这是通过 web 服务器的脚打印完成的。错误处理技术是一个被动过程。标题检查和横幅抓取是收集 web 服务器信息的活动过程。在本章中，我们还了解了解析器 Beautifulsoup。可以从 Beautifulsoup 获得超链接、标记、ID 等部分。在上一节中，您已经看到了一些关于强化 web 服务器的指导原则。如果您遵循这些准则，您可能会使您的 web 服务器难以受到攻击。

在下一章中，您将学习客户端验证和参数调整。您将学习如何生成和检测 DoS 和 DDOS 攻击。********